# Music-Trend-Insight
Using the Kaggle mega dataset to design a model that best predicts popularity of songs using lyrics.

The purpose of this study is to design a model that best predicts popularity of songs using lyrics. As lyrics are one of the most important aspects of music composition, we aimed to predict the popularity of songs using lyrics. Using various models including perceptron, naive bayes, logistic regression, and linear regression, we used different preprocessing techniques to optimize each model and compared the models to find the one that best predicts popularity of songs. Out of all the models, linear regression was our best model with the availability of predicting popularity of songs using different languages of lyrics and the best accuracy. Considering that we did not include other important features that could affect the popularity of songs such as tempo, key, loudness, energy, etc., additional research is needed to improve the models.

Methods:
Perceptron
	The implementation of the perceptron algorithm was used to apply it to the song popularity classification. The words that appear in more than 30 songs are treated as features of the model. There were mainly 4 cases in this study with the use of binary dataset and count dataset for all the English songs in the total dataset and 6 different genres separated in the edm, latin, pop, rap, r&b, and rock datasets. A binary dataset was created by using the word map that was created for the specific case and determining if the presence of the word resulted in the value of 1.0 or 0.0. A count dataset was created by using the word map that was created for the specific case and determining the number of counts of the words. The popularity column was adjusted to 1 being popular with the value greater than or equal to 75 and 0 being not popular with the value less than 75. We splitted the dataset into training and testing sets in a 70:30 ratio respectively. The models were optimized through tuning the hyperparameters, which were the number of iterations and the learning rate. We thought using perceptron would be useful for predicting song popularity based on lyrics because it can handle high-dimensional feature spaces. Also, for the case of the music industry, the model can update the weights incrementally as new songs become available everyday. 

Naive Bayes
We implemented the Naive Bayes algorithm, a popular probabilistic algorithm commonly used for text classification tasks, to predict the popularity of songs based on their lyrics. Naive Bayes assumes that the features (words in this case) are conditionally independent of each other, which makes it computationally efficient and able to handle a large number of features. Our implementation involved preprocessing the lyrics, splitting the data into training and testing sets, and evaluating the performance of the model using the F1 score. We used a Grid Search to find the optimal hyperparameters for a given model. The alpha hyperparameter is used for Laplace smoothing, fit_prior is used to specify whether to learn the class prior probabilities from the training data, and class_prior is used to specify the prior probabilities of the classes. We set the fixed  fit_prior and class_prior and only alpha is vary.  The alpha hyperparameter is used for Laplace smoothing. In Multinomial Naive Bayes, Laplace smoothing is used to avoid the zero-frequency problem, which occurs when a feature does not appear in the training data for a particular class label. The alpha hyperparameter controls the strength of the smoothing, with smaller values resulting in more smoothing.

Logistic Regression
	Logistic regression is a popular classification algorithm used to predict binary outcomes. In our project, we utilized logistic regression to predict the popularity of songs based on their lyrics. We employed a grid search approach to tune the hyperparameters of the model, including the regularization strength parameter C and the type of penalty to apply.  'C' is the regularization parameter that controls the strength of the regularization applied to the model. A small 'C' results in a more constrained model, while a large 'C' results in a less constrained model. 'Penalty' is the type of regularization to be applied to the model, and it can be 'l1', 'l2', 'elasticnet', or None. 'l1' tends to result in sparse solutions, 'l2' tends to result in smooth solutions, and 'elasticnet' is a combination of both.

Linear Regression
	Before implementing the linear regression, we had to perform a PCA on a filtered dataset and plot the results in a 3D scatter plot. The PCA function is used to specify that we want to reduce the dimensionality of the data to three principle components. The method is then used to fit the PCA model to the filtered dataset and transform the data into the new lower-dimensional space. The percentage of total variance explained by the three principle components is calculated and stored in the variable. Finally we had to create a 3D scatter plot of the transformed data, where the three principal components are plotted along the x, y, and z axes. The color of each point is determined by the popularity of the corresponding track in the original dataset, and the title of the plot includes the total explained variance percentage. After performing the PCA, we had to implement the linear regression to predict the popularity of music tracks based on a set of input features. The input dataset is split into two arrays ‘X’, containing the input features and ‘y’, containing the target variable to be predicted, which is the popularity of each track. PCA is then applied to reduce the dimensionality of the input features to three components, and the transformed data is stored. The dataset is split into training and testing sets, and a linear regression model is trained on the training set. The model is then used to predict the popularity of the tracks in the test set, and a scatter plot is created to visualize the relationship between the actual and predicted popularity values. 
